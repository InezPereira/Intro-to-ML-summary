\section*{Latent: Missing Data - UNSUPERVISED}
\subsection*{Mixture modeling}
Model each cluster as probability distr. $P(x|\theta_j)$\\
%Assuming data iid, likelihood is
data iid, likelih.: $P(D|\theta) = \prod_{i=1}^n \sum_{j=1}^k w_j P(x_i|\theta_j)$\\
%Choose parameters to minimize negative log l.\\
$\underset{min}{argmin}L(D;\theta) = \underset{min}{argmin} - \sum_i log \sum_j w_j P(x_i| \theta_j)$

\subsection*{Gaussian-Mixture BC - labeled D! Supervised!}
Estimate class prior $P(y)$; Est. cond. distr. for each class:
$P(x|y) = \sum_{j=1}^{k_y} w_j^{(y)} \mathcal{N}(x; \mu_j^{(y)}, \Sigma_j^{(y)})$\\
Predict:$P(y|x) = \frac{1}{P(x)} p(y) \sum_{j=1}^{k_y} w_j^{(y)} \mathcal{N}(x; \mu_j^{(y)}, \Sigma_j^{(y)})$

\subsection*{EM-algorithm}
E step: $Q(\theta, \theta^{(t)}) = \mathbb{E}_{\textbf{Z} | \textbf{X} = \textbf{x}, \theta^{(t)}}\left[\log P(\textbf{X}, Z | \theta)\right] = \\ \sum_{i=1}^n \mathbb{E}_{\textbf{Z} | \textbf{X} = \textbf{x}_i, \theta^{(t)}}\left[\log P(\textbf{x}_i, Z | \theta)\right] = \\ \sum_{i=1}^n \sum_{z=1}^k P(Z=z|\textbf{x}_i, \theta^{(t)}) \log P(\textbf{X} = \textbf{x}_i, Z=z | \theta)$
with $\gamma_z(\textbf{x}) = P(Z=z|\textbf{X}=\textbf{x}, \theta^{(t-1)})$\\
M step: $\theta^{(t+1)} = \underset{\theta}{argmax} Q(\theta; \theta^{(t-1)})$


\subsection*{Hard-EM algorithm}
Initialize parameters $\theta^{(0)}$\\
For $t=1,2,...$:
%p.: point
Predict most likely class for each data point:
$z_i^{(t)} = \underset{z}{\operatorname{argmax}} P(z|x_i, \theta^{(t-1)})\\
= \underset{z}{\operatorname{argmax}} P(z|\theta^{(t-1)}) P(x_i|z,\theta^{(t-1)})$;\\
MLE (data complete):
$\theta^{(t)} = \underset{\theta}{\operatorname{argmax}} P(D^{(t)}|\theta)$

\subsection*{Soft-EM algorithm: While not converged}
E-step: For each i and j calculate $\gamma_j^{(t)}(x_i)= P(Z=j|x, \mu^{(t-1)}, \Sigma^{(t-1)}, w^{(t-1)}) = \frac{w_jP(x|\Sigma_j, \mu_j)}{\sum_l w_lP(x|\Sigma, \mu_l)}$\\
M-step: Fit clusters to weighted data points:\\

$w_j^{(t)} \leftarrow \frac{1}{n} \sum_{i=1}^n \gamma_j^{(t)} (x_i)$;  %\text{|semi-supervised}\\
$\mu_j^{(t)} \leftarrow \frac{\sum_{i=1}^n \gamma_j^{(t)} (x_i) x_i}{\sum_{i=1}^n \gamma_j^{(t)} (x_i)}\\ %\text{|learning with GMMs } ^t = ^*\\
\Sigma_j^{(t)} \leftarrow \frac{\sum_{i=1}^n \gamma_j^{(t)}(x_i) (x_i - \mu_j^{(t)}) (x_i - \mu_j^{(t)})^T}{\sum_{i=1}^n \gamma_j^{(t)}(x_i)}$\\ %\text{|}\gamma^{(t) = \gamma}$
\textbf{EM for semi-super. learning w/ GMMs:}
%unlabeled points
E: for unlabeled data pts.: $\gamma_j^{(t)}(x_i)$ as before, for pts w/ label $y_i: \gamma_j^{(t)}(x_i) = [j = y_i]$. M: idem

%If enough space put this on summary.
%\subsection*{Log-likelihood}
%$l(\theta) = log P(\mathcal{D})$\\
%$=\sum_{\overset{i=1}{y_i=\times}}^n log P(x_i;\theta) + \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$\\
%$=\sum_{\overset{i=1}{y_i=\times}}^n log \sum_{j=1}^m P(x_i, Y=j;\theta) +$\\
%$ \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$\\
%$=\sum_{\overset{i=1}{y_i=\times}}^n log \sum_{j=1}^m P(x_i|Y=j;\theta)P(Y=j;\theta) +$\\
%$ \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$

%Only until here.
\iffalse
\subsection*{Log-likelihood}
$l(\theta) = log P(\mathcal{D})$ \\
$=\sum_{\overset{i=1}{y_i=\times}}^n log P(x_i;\theta) + \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$\\
$=\sum_{\overset{i=1}{y_i=\times}}^n log \sum_{i=1}^m P(x_i, Y=j;\theta) +$\\
$ \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$\\
$=\sum_{\overset{i=1}{y_i=\times}}^n log \sum_{i=1}^m P(x_i|Y=j;\theta)P(Y=j|\theta) +$\\
$ \sum_{\overset{i=1}{y_i\not=\times}}^n log P(x_i,y_i;\theta)$

\subsection*{Latent variable}
We denote the latent variable indicating the component the point is sampled from by Z, which takes on values in $\{1,...,k\}$.

\subsection*{E-step: Posterior probabilities}
$\gamma_j^t(x_i) = P(Z=j|x_i, \theta_t) = \frac{P(x_i|Z=j, \theta_t) P(Z=j|\theta_t)}{P(x_i;\theta_t)}$

\subsection*{M-step: maximizing expected log likelihood}
$\mathbb{E}_{\gamma^t}[\log P(\mathcal{D;\theta})] = 
\mathbb{E}_{\gamma^t}[\log \Pi_{i=1}^nP(x_i,z_i;\theta)] = $ \\
$\sum_{i=1}^n \mathbb{E}_{\gamma^t}[\log P(x_i,z_i;\theta)] = $ \\
$\sum_{i=1}^n \sum_{j=1}^k \gamma_j^t(x_i) \log (P(x_i|z_i=j;\theta) P(z_i=j;\theta))$ \\
$\theta_{t+1} = \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{\gamma^t}[\log P(\mathcal{D;\theta})]$
\fi